{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQmxJTMZLtzW"
      },
      "source": [
        "# Data crawling with scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "331KxoyiLzDh",
        "outputId": "e373f363-5dae-46ec-ee5e-798c2fa788bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.7.1-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Collecting cryptography>=3.3\n",
            "  Downloading cryptography-38.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 60.1 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting zope.interface>=5.1.0\n",
            "  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 64.2 MB/s \n",
            "\u001b[?25hCollecting service-identity>=18.1.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting Twisted>=18.9.0\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 41.5 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=21.0.0\n",
            "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=446419e6cc6c5b362d069e27aebc240d263ff706d18bcb56ba77a6a5849eb619\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.10.0 constantly-15.1.0 cryptography-38.0.3 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.0.1 zope.interface-5.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-WS95CGMDxH",
        "outputId": "b6ec0db9-419d-4bef-b3d7-992e2381aabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'chilean_data_explore', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/chilean_data_explore\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd chilean_data_explore\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ],
      "source": [
        "!scrapy startproject chilean_data_explore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "6DHjjfHmG1TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pwd\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "L8yHQ4bUGaR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktEKleAVsslZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80da7296-48a3-4bf3-9bd4-79ddd353b377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chilean_data_explore/chilean_data_explore\n",
            "__init__.py  items.py  middlewares.py  pipelines.py  settings.py  spiders\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/chilean_data_explore/chilean_data_explore/')\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chilean_data_explore.chilean_data_explore.items import ChileanDataExploreItem"
      ],
      "metadata": {
        "id": "rUwNiy6yKidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile items.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "class ChileanDataExploreItem(scrapy.Item):\n",
        "    url = scrapy.Field() # str\n",
        "    article_from = scrapy.Field() # str\n",
        "    article_type = scrapy.Field() # str\n",
        "    title = scrapy.Field() # str\n",
        "    publish_date = scrapy.Field() # str\n",
        "    authors = scrapy.Field() # list json\n",
        "    tags = scrapy.Field() # list json\n",
        "    text = scrapy.Field() # list json\n",
        "    text_html = scrapy.Field() # str\n",
        "    images = scrapy.Field() # list json\n",
        "    video = scrapy.Field() # list json\n",
        "    links = scrapy.Field() # list json\n",
        "\n",
        "from scrapy.item import Item, Field\n",
        "class PropertiesItem(Item):\n",
        "     # Primary fields\n",
        "     title = Field()\n",
        "     price = Field()\n",
        "     description = Field()\n",
        "     address = Field()\n",
        "     image_urls = Field()\n",
        "\n",
        "     # Calculated fields\n",
        "     images = Field()\n",
        "     location = Field()\n",
        "     \n",
        "     # Housekeeping fields\n",
        "     url = Field()\n",
        "     project = Field()\n",
        "     spider = Field()\n",
        "     server = Field()\n",
        "     date = Field()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm0AZwn-I4SK",
        "outputId": "83fc527a-15e4-4e9c-9bf0-ca6c1512750a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting items.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b497KTrOMQvL",
        "outputId": "134b65ca-aedb-4a2e-8d99-36f8d93cf066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chilean_data_explore/chilean_data_explore\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "os.chdir('/content/chilean_data_explore/chilean_data_explore/spiders')\n",
        "!mkdir logo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tNWJKg5MoDH",
        "outputId": "42ca836c-9663-48d7-b1c0-1af8cc6f8ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing quotes_spider.py\n"
          ]
        }
      ],
      "source": [
        "#%%writefile -a /chilean_data_explore/chilean_data_explore/spiders/quotes_spider.py\n",
        "%%writefile quotes_spider.py\n",
        "import scrapy\n",
        "from chilean_data_explore.items import ChileanDataExploreItem\n",
        "\n",
        "import time\n",
        "import re\n",
        "\n",
        "class ChinatimesSpider(scrapy.Spider):\n",
        "    name = 'chinatimes'\n",
        "    allowed_domains = ['chinatimes.com']\n",
        "    base_url = 'https://www.chinatimes.com'\n",
        "\n",
        "    url_array = [\n",
        "      'http://observatorio.ministeriodesarrollosocial.gob.cl/encuesta-casen',\n",
        "      'https://www.sii.cl/sobre_el_sii/estadisticas_de_empresas.html',\n",
        "      'https://www.ide.cl/index.php/informacion-territorial/descargar-informacion-territorial',\n",
        "    ]\n",
        "\n",
        "    date_str = str(time.strftime(\"%F\", time.localtime()))\n",
        "\n",
        "    #custom_settings = {\n",
        "    #    'LOG_FILE': 'log/%s-%s.log' % (name, date_str),\n",
        "    #}\n",
        "\n",
        "    def start_requests(self):\n",
        "        list_url = '%s/realtimenews' % (self.base_url)\n",
        "        yield scrapy.Request(url=list_url, callback=self.parse_list)\n",
        "\n",
        "    def parse_list(self, response):\n",
        "        for page_url in response.css('section.article-list>ul>li h3.title>a::attr(href)').getall():\n",
        "            yield scrapy.Request(url=self.base_url + page_url, callback=self.parse_news)\n",
        "\n",
        "    def parse_news(self, response):\n",
        "        item = ChileanDataExploreItem()\n",
        "\n",
        "        item['url'] = response.url\n",
        "        #item['article_from'] = self.name\n",
        "        #item['article_type'] = 'news'\n",
        "\n",
        "        #item['title'] = self._parse_title(response)\n",
        "        #item['publish_date'] = self._parse_publish_date(response)\n",
        "        #item['authors'] = self._parse_authors(response)\n",
        "        #item['tags'] = self._parse_tags(response)\n",
        "        #item['text'] = self._parse_text(response)\n",
        "        #item['text_html'] = self._parse_text_html(response)\n",
        "        #item['images'] = self._parse_images(response)\n",
        "        #item['video'] = self._parse_video(response)\n",
        "        #item['links'] = self._parse_links(response)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def _parse_title(self, response):\n",
        "        return response.css('article.article-box h1.article-title::text').get()\n",
        "\n",
        "    def _parse_publish_date(self, response):\n",
        "        return response.css('article.article-box time::attr(datetime)').get()\n",
        "\n",
        "    def _parse_authors(self, response):\n",
        "        authors = response.css('article.article-box div.author>a::text').getall()\n",
        "        if len(authors) == 0:\n",
        "            authors = [response.css('article.article-box div.author::text').get(default='').strip()]\n",
        "        return authors\n",
        "\n",
        "    def _parse_tags(self, response):\n",
        "        return response.css('article.article-box div.article-hash-tag a::text').getall()\n",
        "\n",
        "    def _parse_text(self, response):\n",
        "        return response.css('article.article-box div.article-body p::text').getall()\n",
        "\n",
        "    def _parse_text_html(self, response):\n",
        "        return response.css('article.article-box div.article-body').get()\n",
        "\n",
        "    def _parse_images(self, response):\n",
        "        images_list = []\n",
        "        images_list.extend(response.css('article.article-box div.main-figure').css('img::attr(src)').getall())\n",
        "        images_list.extend(response.css('article.article-box div.article-body').css('img::attr(src)').getall())\n",
        "        return images_list\n",
        "\n",
        "    def _parse_video(self, response):\n",
        "        return response.css('article.article-box div.article-body iframe::attr(src)').getall()\n",
        "\n",
        "    def _parse_links(self, response):\n",
        "        return response.css('article.article-box div.article-body').css('a::attr(href)').getall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT91tYzFtJRV",
        "outputId": "f374bc6f-522d-4993-a461-fc652152f7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 02:48:28 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:48:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:48:28 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_LOADER_WARN_ONLY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:48:28 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:48:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:48:28 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:48:28 [scrapy.extensions.telnet] INFO: Telnet Password: d2c8d2fa96a8e8c1\n",
            "2022-11-03 02:48:28 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:48:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:48:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:48:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:48:28 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:48:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:48:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/robots.txt> (referer: None)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews> (referer: None)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001737-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001742-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001737-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001737-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001721-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001715-260418> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001742-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001742-260407'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001733-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001725-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001489-260423> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001738-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001754-260409> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001768-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001721-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001721-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001791-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001523-260404> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001715-260418>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001715-260418'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001756-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001783-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001733-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001733-260407'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001725-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001725-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001489-260423>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001489-260423'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001739-260410> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001738-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001738-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001754-260409>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001754-260409'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001768-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001768-260407'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001791-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001791-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001523-260404>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001523-260404'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001756-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001756-260407'}\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001783-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001783-260407'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001761-260405> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:29 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001739-260410>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001739-260410'}\n",
            "2022-11-03 02:48:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001655-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001766-260403> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001777-260412> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001761-260405>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001761-260405'}\n",
            "2022-11-03 02:48:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.chinatimes.com/realtimenews/20221103001741-260407> (referer: https://www.chinatimes.com/realtimenews)\n",
            "2022-11-03 02:48:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001655-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001655-260407'}\n",
            "2022-11-03 02:48:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001766-260403>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001766-260403'}\n",
            "2022-11-03 02:48:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001777-260412>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001777-260412'}\n",
            "2022-11-03 02:48:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.chinatimes.com/realtimenews/20221103001741-260407>\n",
            "{'url': 'https://www.chinatimes.com/realtimenews/20221103001741-260407'}\n",
            "2022-11-03 02:48:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:48:30 [scrapy.extensions.feedexport] INFO: Stored json feed (20 items) in: quotes.json\n",
            "2022-11-03 02:48:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 6498,\n",
            " 'downloader/request_count': 22,\n",
            " 'downloader/request_method_count/GET': 22,\n",
            " 'downloader/response_bytes': 311769,\n",
            " 'downloader/response_count': 22,\n",
            " 'downloader/response_status_count/200': 22,\n",
            " 'elapsed_time_seconds': 1.257454,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 48, 30, 206639),\n",
            " 'httpcompression/response_bytes': 1409854,\n",
            " 'httpcompression/response_count': 22,\n",
            " 'item_scraped_count': 20,\n",
            " 'log_count/DEBUG': 45,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 92987392,\n",
            " 'memusage/startup': 92987392,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 22,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 21,\n",
            " 'scheduler/dequeued/memory': 21,\n",
            " 'scheduler/enqueued': 21,\n",
            " 'scheduler/enqueued/memory': 21,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 48, 28, 949185)}\n",
            "2022-11-03 02:48:30 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy runspider quotes_spider.py -o quotes.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/chilean_data_explore/')\n",
        "!pwd | ls -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUarejI_GFGt",
        "outputId": "b67447b8-1eab-41ac-8739-a5c40932e546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 4 root root 4096 Nov  3 02:48 chilean_data_explore\n",
            "-rw-r--r-- 1 root root  283 Nov  3 02:48 scrapy.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spider for bcentral.cl"
      ],
      "metadata": {
        "id": "w27N3RGwI1F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/chilean_data_explore/chilean_data_explore/spiders/base.py\n",
        "import scrapy\n",
        "from chilean_data_explore.items import PropertiesItem \n",
        "from scrapy.loader import ItemLoader\n",
        "import datetime\n",
        "import socket\n",
        "\n",
        "class BasicSpider(scrapy.Spider):\n",
        "    name = 'base'\n",
        "    allowed_domains = ['si3.bcentral.cl']\n",
        "    start_urls = ['https://si3.bcentral.cl/siete']\n",
        "\n",
        "    def parse(self, response):\n",
        "      loader_item = ItemLoader(item=PropertiesItem(), response=response)\n",
        "      loader_item.add_xpath('title','//title/text()')\n",
        "      loader_item.add_xpath('price','//*[@itemprop=\"price\"][1]/text()', re='[.0-9]+')\n",
        "      loader_item.add_xpath('description','//*[contains(@href, \"html\")]')\n",
        "      loader_item.add_value('server', socket.gethostname())\n",
        "      loader_item.add_value('date', datetime.datetime.now())\n",
        "      return loader_item.load_item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXvokRiuHZ93",
        "outputId": "df94ac86-4a85-4be1-fd51-616c885f9ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/chilean_data_explore/chilean_data_explore/spiders/base.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNcPAxDZJeSE",
        "outputId": "c1fca0c0-ad7a-4434-d05f-a9572098a49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 02:48:31 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:48:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:48:31 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:48:31 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:48:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:48:31 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:48:31 [scrapy.extensions.telnet] INFO: Telnet Password: c96b18eb15db4956\n",
            "2022-11-03 02:48:31 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:48:31 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:48:31 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:48:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:48:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 02:48:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:33 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Attempting to acquire lock 140541561488272 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Lock 140541561488272 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Attempting to acquire lock 140541546248976 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Lock 140541546248976 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
            "2022-11-03 02:48:35 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Attempting to release lock 140541546248976 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Lock 140541546248976 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Attempting to release lock 140541561488272 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [filelock] DEBUG: Lock 140541561488272 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/siete> (referer: None)\n",
            "2022-11-03 02:48:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://si3.bcentral.cl/siete>\n",
            "{'date': [datetime.datetime(2022, 11, 3, 2, 48, 35, 413524)],\n",
            " 'description': ['<a target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index.html\">Glosario</a>',\n",
            "                 '<a target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index_faq.html\">Preguntas '\n",
            "                 'Frecuentes</a>',\n",
            "                 '<a class=\"nav-link\" target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/Metodolog_BDE.html\">Metodologías</a>',\n",
            "                 '<a class=\"nav-link\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/BDE_movil.html\">BDE '\n",
            "                 'Móvil</a>'],\n",
            " 'server': ['cd173063d6cb'],\n",
            " 'title': ['Base de Datos Estadísticos ']}\n",
            "2022-11-03 02:48:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:48:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 674,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 38967,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'elapsed_time_seconds': 3.871621,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 48, 35, 426798),\n",
            " 'item_scraped_count': 1,\n",
            " 'log_count/DEBUG': 29,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 93257728,\n",
            " 'memusage/startup': 93257728,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 48, 31, 555177)}\n",
            "2022-11-03 02:48:35 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl base -o items.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npKp6C2eADMv",
        "outputId": "035737a4-02d4-4a4c-a206-8cb7b2d37701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 02:48:38 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:48:38 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:48:38 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:48:38 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:48:38 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:48:38 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:48:38 [scrapy.extensions.telnet] INFO: Telnet Password: a6dafb744dd514e9\n",
            "2022-11-03 02:48:38 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:48:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:48:38 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:48:38 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:48:38 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:48:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:48:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:48:39 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 02:48:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:39 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:41 [filelock] DEBUG: Attempting to acquire lock 140056703081808 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:41 [filelock] DEBUG: Lock 140056703081808 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:41 [filelock] DEBUG: Attempting to release lock 140056703081808 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:41 [filelock] DEBUG: Lock 140056703081808 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/siete> (referer: None)\n",
            "2022-11-03 02:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://si3.bcentral.cl/siete>\n",
            "{'date': [datetime.datetime(2022, 11, 3, 2, 48, 41, 193152)],\n",
            " 'description': ['<a target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index.html\">Glosario</a>',\n",
            "                 '<a target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index_faq.html\">Preguntas '\n",
            "                 'Frecuentes</a>',\n",
            "                 '<a class=\"nav-link\" target=\"_blank\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/Metodolog_BDE.html\">Metodologías</a>',\n",
            "                 '<a class=\"nav-link\" '\n",
            "                 'href=\"https://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/BDE_movil.html\">BDE '\n",
            "                 'Móvil</a>'],\n",
            " 'server': ['cd173063d6cb'],\n",
            " 'title': ['Base de Datos Estadísticos ']}\n",
            "2022-11-03 02:48:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:48:41 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: items.csv\n",
            "2022-11-03 02:48:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 674,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 38967,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'elapsed_time_seconds': 2.629536,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 48, 41, 199235),\n",
            " 'item_scraped_count': 1,\n",
            " 'log_count/DEBUG': 23,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 93405184,\n",
            " 'memusage/startup': 93405184,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 48, 38, 569699)}\n",
            "2022-11-03 02:48:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy parse --spider=base https://si3.bcentral.cl/siete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQI8QVlgDB0Y",
        "outputId": "8bf14b61-b70a-4c2a-d61c-fe27299c6293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 02:48:43 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:48:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:48:43 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:48:43 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:48:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:48:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:48:43 [scrapy.extensions.telnet] INFO: Telnet Password: 7ec03abe3ad8eabb\n",
            "2022-11-03 02:48:43 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:48:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:48:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:48:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:48:43 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:48:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:48:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:48:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 02:48:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:44 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:45 [filelock] DEBUG: Attempting to acquire lock 140404634904144 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:45 [filelock] DEBUG: Lock 140404634904144 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:45 [filelock] DEBUG: Attempting to release lock 140404634904144 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:45 [filelock] DEBUG: Lock 140404634904144 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/siete> (referer: None)\n",
            "2022-11-03 02:48:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:48:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 674,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 38967,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'elapsed_time_seconds': 2.649237,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 48, 45, 963857),\n",
            " 'log_count/DEBUG': 22,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 93450240,\n",
            " 'memusage/startup': 93450240,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 48, 43, 314620)}\n",
            "2022-11-03 02:48:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "\n",
            ">>> STATUS DEPTH LEVEL 1 <<<\n",
            "# Scraped Items  ------------------------------------------------------------\n",
            "[{\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [datetime.datetime(\u001b[34m2022\u001b[39;49;00m, \u001b[34m11\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m48\u001b[39;49;00m, \u001b[34m45\u001b[39;49;00m, \u001b[34m960084\u001b[39;49;00m)],\n",
            "  \u001b[33m'\u001b[39;49;00m\u001b[33mdescription\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33m<a target=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m_blank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mhref=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index.html\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>Glosario</a>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33m<a target=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m_blank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mhref=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/index_faq.html\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>Preguntas \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mFrecuentes</a>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33m<a class=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mnav-link\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m target=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m_blank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mhref=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/Metodolog_BDE.html\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>Metodologías</a>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33m<a class=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mnav-link\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mhref=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://si3.bcentral.cl/estadisticas/Principal1/enlaces/aplicaciones/BDE_movil.html\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>BDE \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
            "                  \u001b[33m'\u001b[39;49;00m\u001b[33mMóvil</a>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
            "  \u001b[33m'\u001b[39;49;00m\u001b[33mserver\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mcd173063d6cb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
            "  \u001b[33m'\u001b[39;49;00m\u001b[33mtitle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mBase de Datos Estadísticos \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]}]\n",
            "\n",
            "# Requests  -----------------------------------------------------------------\n",
            "[]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spider for two-direction-movement"
      ],
      "metadata": {
        "id": "pA5FqcRkISfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/chilean_data_explore/chilean_data_explore/spiders/manual.py\n",
        "import scrapy\n",
        "from chilean_data_explore.items import PropertiesItem \n",
        "from scrapy.loader import ItemLoader\n",
        "import datetime\n",
        "import socket\n",
        "from scrapy.http import Request\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "class BasicSpider(scrapy.Spider):\n",
        "    name = 'manual'\n",
        "    allowed_domains = ['si3.bcentral.cl']\n",
        "    start_urls = ['https://si3.bcentral.cl/Siete']\n",
        "\n",
        "    def parse(self, response):\n",
        "      next_selector=response.xpath('//a[starts-with(@onclick, \"SetCapitu\")]//@onclick').extract()\n",
        "      collec=[]\n",
        "      for string in next_selector:\n",
        "        string = string.replace(\"SetCapitulo(\",\"\") \n",
        "        string = string.replace(\")\",\"\") \n",
        "        string = string.replace(\"'\",\"\") \n",
        "        string = string.replace(\" \",\"\") \n",
        "        string = \"/\".join(string.split(',')) \n",
        "        string = \"Siete/ES/Siete/Cuadro/\" + string.strip()\n",
        "        collec.append(string)\n",
        "      next_selector = collec\n",
        "\n",
        "      for url in next_selector:\n",
        "        x = urljoin(response.url, url)\n",
        "        yield Request(x, callback=self.parse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5zxQVxlaOoU",
        "outputId": "2c70aab6-760f-44bc-e2f0-bcc703db213c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/chilean_data_explore/chilean_data_explore/spiders/manual.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl manual \n",
        "!scrapy parse --spider=manual https://si3.bcentral.cl/Siete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-64ZVxfaC-b",
        "outputId": "2e174a6a-fac4-4498-dbfe-6bf4d4005290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 02:48:47 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:48:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:48:47 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:48:47 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:48:47 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:48:47 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:48:48 [scrapy.extensions.telnet] INFO: Telnet Password: 5c39eded37782e89\n",
            "2022-11-03 02:48:48 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:48:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:48:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:48:48 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:48:48 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:48:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:48:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:48:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 02:48:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:50 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 02:48:51 [filelock] DEBUG: Attempting to acquire lock 139962231680848 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:51 [filelock] DEBUG: Lock 139962231680848 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:51 [filelock] DEBUG: Attempting to release lock 139962231680848 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:51 [filelock] DEBUG: Lock 139962231680848 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete> (referer: None)\n",
            "2022-11-03 02:48:51 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
            "2022-11-03 02:48:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:48:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:49:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:49:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:49:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 02:49:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:49:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 8843,\n",
            " 'downloader/request_count': 19,\n",
            " 'downloader/request_method_count/GET': 19,\n",
            " 'downloader/response_bytes': 4403066,\n",
            " 'downloader/response_count': 19,\n",
            " 'downloader/response_status_count/200': 18,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'dupefilter/filtered': 16,\n",
            " 'elapsed_time_seconds': 12.827009,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 49, 1, 13928),\n",
            " 'log_count/DEBUG': 39,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 93507584,\n",
            " 'memusage/startup': 93507584,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 18,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 17,\n",
            " 'scheduler/dequeued/memory': 17,\n",
            " 'scheduler/enqueued': 17,\n",
            " 'scheduler/enqueued/memory': 17,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 48, 48, 186919)}\n",
            "2022-11-03 02:49:01 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "2022-11-03 02:49:02 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 02:49:02 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 02:49:02 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 02:49:02 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 02:49:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 02:49:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 02:49:02 [scrapy.extensions.telnet] INFO: Telnet Password: 4424729a9f846f06\n",
            "2022-11-03 02:49:03 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 02:49:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 02:49:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 02:49:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 02:49:03 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 02:49:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 02:49:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 02:49:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 02:49:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:04 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 02:49:05 [filelock] DEBUG: Attempting to acquire lock 140666737213840 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:49:05 [filelock] DEBUG: Lock 140666737213840 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:49:05 [filelock] DEBUG: Attempting to release lock 140666737213840 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:49:05 [filelock] DEBUG: Lock 140666737213840 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 02:49:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete> (referer: None)\n",
            "2022-11-03 02:49:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 02:49:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 674,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 38967,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'elapsed_time_seconds': 2.520049,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 2, 49, 5, 698139),\n",
            " 'log_count/DEBUG': 22,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 93609984,\n",
            " 'memusage/startup': 93609984,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 2, 49, 3, 178090)}\n",
            "2022-11-03 02:49:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "\n",
            ">>> STATUS DEPTH LEVEL 1 <<<\n",
            "# Scraped Items  ------------------------------------------------------------\n",
            "[]\n",
            "\n",
            "# Requests  -----------------------------------------------------------------\n",
            "[<GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T>,\n",
            " <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001>]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawlspider for two-direction-movement"
      ],
      "metadata": {
        "id": "RDuADs6crNO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider -t crawl automatic web"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQFNDfdgr4y4",
        "outputId": "112d9db1-2ba4-416e-ae4a-eba468d9cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'automatic' using template 'crawl' in module:\n",
            "  chilean_data_explore.spiders.automatic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/chilean_data_explore/chilean_data_explore/spiders/automatic.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from chilean_data_explore.items import ChileanDataExploreItem\n",
        "\n",
        "\n",
        "class AutomaticSpider(CrawlSpider):\n",
        "    name = 'automatic'\n",
        "    allowed_domains = ['si3.bcentral.cl']\n",
        "    start_urls = ['https://si3.bcentral.cl/Siete']\n",
        "\n",
        "    rules = (\n",
        "        #vertical\n",
        "        Rule(LinkExtractor(allow='Siete'), callback='parse_item', follow=True),\n",
        "    )\n",
        "    \n",
        "    def parse_item(self, response):\n",
        "        exist = response.xpath('//a[starts-with(@onclick, \"SetCapitu\")]').extract_first()\n",
        "\n",
        "        if exist:\n",
        "          title = response.xpath('//a[starts-with(@onclick, \"SetCapitu\")]//@onclick')\n",
        "          title = [ self.start_urls[0]+\"/ES/Siete/Cuadro/\" +self.url_title(t.get())  for t in title]\n",
        "          #title = [ t.get()  for t in title]\n",
        "          print( title)\n",
        "\n",
        "          item = ChileanDataExploreItem()\n",
        "\n",
        "          item['url'] = \"response.url\"\n",
        "          item['article_from'] = \"self.name\"\n",
        "          item['title'] = title\n",
        "          item['publish_date'] = \"self._parse_publish_date(response)\"\n",
        "\n",
        "          #yield {          'title': title          }\n",
        "          yield item\n",
        "          \n",
        "          print(\"Testing > \", response.url )\n",
        "\n",
        "          for url_next_page in title:\n",
        "            yield response.follow(url_next_page, callback=self.parse_item)\n",
        "\n",
        "        else:\n",
        "          print(response.url)\n",
        "\n",
        "    def url_title(self,t):\n",
        "        replacements = [\n",
        "          (\"'\",\"\"),\n",
        "          (\")\",\"\"),\n",
        "          (\"(\",\"\"),\n",
        "          (\", \",\"/\"),\n",
        "          (\"SetCapitulo\",\"\"),\n",
        "        ]\n",
        "        for x,y in replacements:\n",
        "          t=t.replace(x,y)\n",
        "\n",
        "        return t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5vUvve2f-VA",
        "outputId": "75dc2d7e-c07e-4a76-d4e0-11b223c74be4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/chilean_data_explore/chilean_data_explore/spiders/automatic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl automatic -O data_bcentral.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVO9WJHxnam2",
        "outputId": "26392de7-23ed-4f95-d0dd-a64a30c81365"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 03:02:47 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 03:02:47 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 03:02:47 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 03:02:47 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 03:02:47 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 03:02:47 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 03:02:47 [scrapy.extensions.telnet] INFO: Telnet Password: b46a409418fe8459\n",
            "2022-11-03 03:02:47 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 03:02:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 03:02:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 03:02:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 03:02:47 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 03:02:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 03:02:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 03:02:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 03:02:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:49 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 03:02:50 [filelock] DEBUG: Attempting to acquire lock 140000760457488 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 03:02:50 [filelock] DEBUG: Lock 140000760457488 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 03:02:50 [filelock] DEBUG: Attempting to release lock 140000760457488 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 03:02:50 [filelock] DEBUG: Lock 140000760457488 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 03:02:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete> (referer: None)\n",
            "2022-11-03 03:02:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete> (referer: https://si3.bcentral.cl/)\n",
            "['https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T', 'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001']\n",
            "2022-11-03 03:02:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://si3.bcentral.cl/Siete>\n",
            "{'article_from': 'self.name',\n",
            " 'publish_date': 'self._parse_publish_date(response)',\n",
            " 'title': ['https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T',\n",
            "           'https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001'],\n",
            " 'url': 'response.url'}\n",
            "Testing >  https://si3.bcentral.cl/Siete\n",
            "2022-11-03 03:02:51 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
            "2022-11-03 03:02:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC\n",
            "2022-11-03 03:02:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2\n",
            "2022-11-03 03:02:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO\n",
            "2022-11-03 03:02:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01\n",
            "2022-11-03 03:02:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1\n",
            "2022-11-03 03:02:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008\n",
            "2022-11-03 03:02:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 03:02:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_BDP/MN_BDP42/BP6M_RES01\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01\n",
            "2022-11-03 03:02:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_EI/MN_EI11/EI_CREC_TRI\n",
            "2022-11-03 03:02:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1\n",
            "2022-11-03 03:02:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO> (referer: https://si3.bcentral.cl/)\n",
            "2022-11-03 03:02:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01\n",
            "2022-11-03 03:02:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001\n",
            "2022-11-03 03:02:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T\n",
            "2022-11-03 03:03:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01\n",
            "2022-11-03 03:03:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2> (referer: https://si3.bcentral.cl/)\n",
            "https://si3.bcentral.cl/Siete/ES/Siete/Cuadro/CAP_DYB/MN_ESTAD_MON55/EM_BMAM2\n",
            "2022-11-03 03:03:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 03:03:00 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data_bcentral.csv\n",
            "2022-11-03 03:03:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 9297,\n",
            " 'downloader/request_count': 20,\n",
            " 'downloader/request_method_count/GET': 20,\n",
            " 'downloader/response_bytes': 4436259,\n",
            " 'downloader/response_count': 20,\n",
            " 'downloader/response_status_count/200': 19,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'dupefilter/filtered': 17,\n",
            " 'elapsed_time_seconds': 12.897458,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 3, 3, 0, 971937),\n",
            " 'item_scraped_count': 1,\n",
            " 'log_count/DEBUG': 41,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 107888640,\n",
            " 'memusage/startup': 107888640,\n",
            " 'request_depth_max': 2,\n",
            " 'response_received_count': 19,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 18,\n",
            " 'scheduler/dequeued/memory': 18,\n",
            " 'scheduler/enqueued': 18,\n",
            " 'scheduler/enqueued/memory': 18,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 3, 2, 48, 74479)}\n",
            "2022-11-03 03:03:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider -t crawl tencent web"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm3B5BprimdP",
        "outputId": "25f7f365-c75b-453b-9279-5a88032e19f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'tencent' using template 'crawl' in module:\n",
            "  chilean_data_explore.spiders.tencent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/chilean_data_explore/chilean_data_explore/spiders/tencent.py\n",
        "import scrapy\n",
        "\n",
        "#load class CrawlSpider and Rule\n",
        "from scrapy.spiders import CrawlSpider,Rule\n",
        "\n",
        "#load the link rules to match satisfiled links\n",
        "from scrapy.linkextractors import LinkExtractor \n",
        "\n",
        "#from tenSpider.items import TenspiderItem \n",
        "\n",
        "class TencentSpider(CrawlSpider):\n",
        "    name = 'tencent'\n",
        "    allowed_domains = ['si3.bcentral.cl']\n",
        "    start_urls = ['https://si3.bcentral.cl/Siete']\n",
        "\n",
        "    # the extract function in response,return a list match the rule\n",
        "    list_pagelink=LinkExtractor(restrict_xpaths='//a[starts-with(@onclick, \"SetCapitu\")]//@onclick' )\n",
        "    \n",
        "    #follow item will be used to Recursive crawling\n",
        "    rules=[\n",
        "        # get the link in pagelink, send the request one by one,\n",
        "        # call the specified function to process \n",
        "        Rule(list_pagelink,callback=\"parseTencent\",follow=True)\n",
        "    ]\n",
        "\n",
        "    def list_url(self,response):\n",
        "        list_url_ = [ \"/ES/Siete/Cuadro/\" +self.url_title(t.get())  for t in response.xpath('//a[starts-with(@onclick, \"SetCapitu\")]//@onclick') ]\n",
        "        print(list_url_)\n",
        "        return list_url_\n",
        "\n",
        "    def url_title(self,t):\n",
        "        replacements = [\n",
        "          (\"'\",\"\"),\n",
        "          (\")\",\"\"),\n",
        "          (\"(\",\"\"),\n",
        "          (\", \",\"/\"),\n",
        "          (\"SetCapitulo\",\"\"),\n",
        "        ]\n",
        "        for x,y in replacements:\n",
        "          t=t.replace(x,y)\n",
        "\n",
        "        return t\n",
        "\n",
        "    # specified process function\n",
        "    def parseTencent(self,response):\n",
        "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<\",response.url)\n",
        "        for each in response.xpath(\"//tr[@class='even'] | //tr[@class='odd']\"):\n",
        "            item={}#TenspiderItem()\n",
        "            #store the data into dict item\n",
        "            #extract will transform the data into unicode string\n",
        "            item['positionname']=each.xpath('./td[1]/a/text()').extract()[0]\n",
        "            item['positionlink']=each.xpath('./td[1]/a/@href').extract()[0]\n",
        "            item['positionType']=each.xpath('./td[2]/text()').extract()[0]\n",
        "            item['peopleNum']=each.xpath('./td[3]/text()').extract()[0]\n",
        "            item['workLocation']=each.xpath('./td[4]/text()').extract()[0]\n",
        "            item['publishTime']=each.xpath('./td[5]/text()').extract()[0]\n",
        "\n",
        "            print(item)\n",
        "            yield item"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7-EFjgWiukh",
        "outputId": "733a5376-1009-4d85-bd7e-bc005c85cb74"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/chilean_data_explore/chilean_data_explore/spiders/tencent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl tencent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTzd8Dejmfj",
        "outputId": "9560e33f-799f-4043-cf2d-b0bcce88a640"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-03 04:21:05 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: chilean_data_explore)\n",
            "2022-11-03 04:21:05 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.3, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-11-03 04:21:05 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'chilean_data_explore',\n",
            " 'NEWSPIDER_MODULE': 'chilean_data_explore.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['chilean_data_explore.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2022-11-03 04:21:05 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-11-03 04:21:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-11-03 04:21:05 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-11-03 04:21:05 [scrapy.extensions.telnet] INFO: Telnet Password: 2707691d12725df9\n",
            "2022-11-03 04:21:05 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-11-03 04:21:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-11-03 04:21:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-11-03 04:21:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-11-03 04:21:05 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-11-03 04:21:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-11-03 04:21:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-11-03 04:21:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://si3.bcentral.cl/ErrorPage.html> from <GET https://si3.bcentral.cl/robots.txt>\n",
            "2022-11-03 04:21:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/ErrorPage.html> (referer: None)\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 21 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 28 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 39 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 55 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 84 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 90 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 91 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 95 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:06 [protego] DEBUG: Rule at line 96 without any user agent to enforce it on.\n",
            "2022-11-03 04:21:08 [filelock] DEBUG: Attempting to acquire lock 139663136187536 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 04:21:08 [filelock] DEBUG: Lock 139663136187536 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 04:21:08 [filelock] DEBUG: Attempting to release lock 139663136187536 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 04:21:08 [filelock] DEBUG: Lock 139663136187536 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-11-03 04:21:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://si3.bcentral.cl/Siete> (referer: None)\n",
            "2022-11-03 04:21:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://si3.bcentral.cl/Siete> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/defer.py\", line 254, in aiter_errback\n",
            "    yield await it.__anext__()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/python.py\", line 366, in __anext__\n",
            "    return await self.data.__anext__()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/python.py\", line 347, in _async_chain\n",
            "    async for o in as_async_generator(it):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/asyncgen.py\", line 14, in as_async_generator\n",
            "    async for r in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/python.py\", line 366, in __anext__\n",
            "    return await self.data.__anext__()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/python.py\", line 347, in _async_chain\n",
            "    async for o in as_async_generator(it):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/asyncgen.py\", line 14, in as_async_generator\n",
            "    async for r in it:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/core/spidermw.py\", line 90, in process_async\n",
            "    async for r in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 32, in process_spider_output_async\n",
            "    async for r in result or ():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/core/spidermw.py\", line 90, in process_async\n",
            "    async for r in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spidermiddlewares/referer.py\", line 339, in process_spider_output_async\n",
            "    async for r in result or ():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/core/spidermw.py\", line 90, in process_async\n",
            "    async for r in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 31, in process_spider_output_async\n",
            "    async for r in result or ():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/core/spidermw.py\", line 90, in process_async\n",
            "    async for r in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spidermiddlewares/depth.py\", line 36, in process_spider_output_async\n",
            "    async for r in result or ():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/core/spidermw.py\", line 90, in process_async\n",
            "    async for r in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiders/crawl.py\", line 125, in _parse_response\n",
            "    for request_or_item in self._requests_to_follow(response):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiders/crawl.py\", line 98, in _requests_to_follow\n",
            "    links = [lnk for lnk in rule.link_extractor.extract_links(response)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/linkextractors/lxmlhtml.py\", line 162, in extract_links\n",
            "    links = self._extract_links(doc, response.url, response.encoding, base_url)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/linkextractors/__init__.py\", line 132, in _extract_links\n",
            "    return self.link_extractor._extract_links(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/linkextractors/lxmlhtml.py\", line 64, in _extract_links\n",
            "    for el, attr, attr_val in self._iter_links(selector.root):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/linkextractors/lxmlhtml.py\", line 52, in _iter_links\n",
            "    for el in document.iter(etree.Element):\n",
            "AttributeError: 'str' object has no attribute 'iter'\n",
            "2022-11-03 04:21:08 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-11-03 04:21:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 674,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 38967,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/302': 1,\n",
            " 'elapsed_time_seconds': 2.775103,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 11, 3, 4, 21, 8, 354379),\n",
            " 'log_count/DEBUG': 22,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 108322816,\n",
            " 'memusage/startup': 108322816,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/AttributeError': 1,\n",
            " 'start_time': datetime.datetime(2022, 11, 3, 4, 21, 5, 579276)}\n",
            "2022-11-03 04:21:08 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up selenium enviroment"
      ],
      "metadata": {
        "id": "mq9RIExW_JYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmhks0_X0TJK",
        "outputId": "393cf2a8-f0ea-4dab-cef3-5408302a52b0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.39\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [98.9 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [992 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,196 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,124 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,472 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,267 kB]\n",
            "Fetched 16.4 MB in 4s (3,698 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 91.7 MB of archives.\n",
            "After this operation, 309 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 105.0.5195.102-0ubuntu0.18.04.1 [1,156 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 105.0.5195.102-0ubuntu0.18.04.1 [80.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 105.0.5195.102-0ubuntu0.18.04.1 [5,097 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 105.0.5195.102-0ubuntu0.18.04.1 [5,320 kB]\n",
            "Fetched 91.7 MB in 5s (17.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_105.0.5195.102-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n",
            "\u001b[K     |████████████████████████████████| 995 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[K     |████████████████████████████████| 384 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n",
            "Collecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.0.0 h11-0.14.0 outcome-1.2.0 selenium-4.5.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.12 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "class selenium_scrapy:\n",
        "  url_list=[]\n",
        "  def driversetup(self):\n",
        "      options = webdriver.ChromeOptions()\n",
        "      #run Selenium in headless mode\n",
        "      options.add_argument('--headless')\n",
        "      options.add_argument('--no-sandbox')\n",
        "      #overcome limited resource problems\n",
        "      options.add_argument('--disable-dev-shm-usage')\n",
        "      options.add_argument(\"lang=en\")\n",
        "      #open Browser in maximized mode\n",
        "      options.add_argument(\"start-maximized\")\n",
        "      #disable infobars\n",
        "      options.add_argument(\"disable-infobars\")\n",
        "      #disable extension\n",
        "      options.add_argument(\"--disable-extensions\")\n",
        "      options.add_argument(\"--incognito\")\n",
        "      options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "      \n",
        "      driver = webdriver.Chrome(options=options)\n",
        "\n",
        "      driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\")\n",
        "\n",
        "      return driver\n",
        "    \n",
        "  def pagesource(self,url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "      #soup = BeautifulSoup(driver.page_source)\n",
        "      soup=driver.find_elements(By.XPATH, '//a[starts-with(@onclick, \"SetCapitu\")]' )\n",
        "      self.url_list = []\n",
        "      for a in soup:\n",
        "        self.url_list.append(self.url_title(a.get_attribute('onclick')))\n",
        "\n",
        "      driver.close()\n",
        "\n",
        "  def url_title(self,t):\n",
        "      replacements = [\n",
        "        (\"'\",\"\"),\n",
        "        (\")\",\"\"),\n",
        "        (\"(\",\"\"),\n",
        "        (\", \",\"/\"),\n",
        "        (\"SetCapitulo\",\"\"),\n",
        "      ]\n",
        "      for x,y in replacements:\n",
        "        t=t.replace(x,y)\n",
        "      return t"
      ],
      "metadata": {
        "id": "zJntoVOa0UYh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_crawler = selenium_scrapy()\n",
        "data_crawler.driversetup()\n",
        "url='https://si3.bcentral.cl/Siete'\n",
        "data_crawler.pagesource(url)\n",
        "\n",
        "print(data_crawler.url_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH1tzX5x0gVv",
        "outputId": "22164436-e525-4880-ff72-ffd114e29a0a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC', 'CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO', 'CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1', 'CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO', 'CAP_DYB/MN_ESTAD_MON55/EM_BMAM2', 'CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01', 'CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01', 'CAP_BDP/MN_BDP42/BP6M_RES01', 'CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2', 'CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01', 'CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008', 'CAP_EI/MN_EI11/EI_CREC_TRI', 'CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1', 'CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01', 'CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T', 'CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001', 'CAP_ESTADIST_MACRO/MN_EST_MACRO_IV/PEM_TC', 'CAP_PRECIOS/MN_CAP_PRECIOS/UF_IVP_DIARIO', 'CAP_TASA_INTERES/MN_TASA_INTERES_09/TPM_C1', 'CAP_TIPO_CAMBIO/MN_TIPO_CAMBIO4/DOLAR_OBS_ADO', 'CAP_DYB/MN_ESTAD_MON55/EM_BMAM2', 'CAP_DERYSPOT/MN_DERYSPOT/DER_MON_01', 'CAP_CCNN/MN_CCNN76/CCNN2018_IMACEC_01', 'CAP_BDP/MN_BDP42/BP6M_RES01', 'CAP_EMP_REM_DEM/MN_EMP_REM_DEM13/ED_TDNRM2', 'CAP_EXP_ECO/MN_EXP_EC11/EXE_BCCH_01', 'CAP_IND_SEC/MN_IND_SEC20/IS_GENERAL_PROPIEDAD_2008', 'CAP_EI/MN_EI11/EI_CREC_TRI', 'CAP_FIN_PUB/MN_FIN_PUB_1/GOB_TOT_1', 'CAP_ESTADIST_GENERO/MN_GENERO1/EST_GEN_POB_01', 'CAP_ESTADIST_REGIONAL/MN_REGIONAL1/CCNN2018_PIB_REGIONAL_T', 'CAP_ESTADIST_EXPERIM/MN_EXPERIM01/EST_EXP_001']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fr-sbmiard"
      },
      "source": [
        "# Storing crawled data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Xs4DvVhptd",
        "outputId": "2ecc2cc0-c446-4ebc-ac26-c6d98efa7744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.24.90)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.90 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.27.90)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.90->boto3) (1.26.12)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.90->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.90->boto3) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qTH9y60mktG"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import os\n",
        "import requests\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVJmdLnhlp2g"
      },
      "outputs": [],
      "source": [
        "dynamo_client  =  boto3.resource(service_name = 'data_crawled_dydb',region_name = 'us-east-1',\n",
        "              aws_access_key_id = 'AKIA3BS5NFXXXXXXX',\n",
        "              aws_secret_access_key = 'qfGTJL28HrqcbhKCM0t//xxx7gTGG4iNrv3/d94Lsp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3_jZTMKl1QE"
      },
      "outputs": [],
      "source": [
        "product_table = dynamo_client.Table('institution')\n",
        "product_table.table_status\n",
        "\n",
        "product_table_link = dynamo_client.Table('link_collection')\n",
        "product_table_link.table_status\n",
        "\n",
        "product_table_file = dynamo_client.Table('file_collection')\n",
        "product_table_file.table_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sPF8vXMlqko"
      },
      "outputs": [],
      "source": [
        "dynamo_client.get_available_subresources()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60fizkKMlXAY"
      },
      "outputs": [],
      "source": [
        "def query_police_department_record_by_guid(guid):  \n",
        "    db = dynamodb_resource()\n",
        "    extra_msg = {\"region_name\": REGION, \"aws_service\": \"dynamodb\", \n",
        "        \"police_department_table\":POLICE_DEPARTMENTS_TABLE,\n",
        "        \"guid\":guid}\n",
        "    log.info(f\"Get PD record by GUID\", extra=extra_msg)\n",
        "    pd_table = db.Table(POLICE_DEPARTMENTS_TABLE)\n",
        "    response = pd_table.get_item(\n",
        "        Key={\n",
        "            'guid': guid\n",
        "            }\n",
        "    )\n",
        "    return response['Item']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-gDjN1cpY01"
      },
      "outputs": [],
      "source": [
        "print(query_police_department_record_by_guid(\"jlkdajfldskj1312312\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0_pb1WDLr-3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gB_b1yYNYbE"
      },
      "source": [
        "#References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCw9Gh1nib7k"
      },
      "source": [
        "> DynamoDB and its purposes\n",
        "\n",
        "*   [A one size fits all database doesn't fit anyone\n",
        "](https://www.allthingsdistributed.com/2018/06/purpose-built-databases-in-aws.html)\n",
        "\n",
        "*   [Amazon DynamoDB](https://aws.amazon.com/dynamodb/)\n",
        "\n",
        "*   [Scaling globally with the new AWS](https://www.allthingsdistributed.com/2022/08/aws-launches-middle-east-uae-region.html)\n",
        "\n",
        "\n",
        "> Interfaces for reliable connections\n",
        "\n",
        "*   [DynamoDB and the AWS SDKs](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.html\n",
        ")\n",
        "\n",
        "*   [Boto3 and Amazon DynamoDB](https://www.section.io/engineering-education/python-boto3-and-amazon-dynamodb-programming-tutorial/)\n",
        "\n",
        "*   [DynamoDb in Python using BOTO3](https://www.analyticsvidhya.com/blog/2022/05/working-with-dynamodb-in-python-using-boto3/)\n",
        "\n",
        "> Scrapy \n",
        "\n",
        "* [Google Colab tips: using both %%writefile magic and %%javascript magic in the same cell\n",
        "](https://stephencowchau.medium.com/google-colab-tips-using-both-writefile-magic-and-javascript-magic-in-the-same-cell-7820e508e455)\n",
        "\n",
        "* [Scrapy - User Agents and Proxies](https://scrapeops.io/python-scrapy-playbook/scrapy-beginners-guide-user-agents-proxies/)\n",
        "\n",
        "> Xpath\n",
        "\n",
        "* [Parsing HTML with Xpath](https://scrapfly.io/blog/parsing-html-with-xpath/)\n",
        "\n",
        "* [Scrapy - User Agents and Proxies](https://scrapeops.io/python-scrapy-playbook/scrapy-beginners-guide-user-agents-proxies/)\n",
        "\n",
        "* [XPath tester](https://extendsclass.com/xpath-tester.html)\n",
        "\n",
        "*   [XPath tester codebeautify](https://codebeautify.org/Xpath-Tester)\n",
        "\n",
        "*   [Xpath for python: is xpath underappreciated?](https://towardsdatascience.com/xpath-for-python-89f4423415e0)\n",
        "\n",
        "> Selenium\n",
        "\n",
        "*   [Selenium webdriver in colab](https://blog.devgenius.io/use-selenium-webdriver-in-google-colab-d5f2dba1d9f5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3fr-sbmiard"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPe93C6xUrAgbXfj5sknOTE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}