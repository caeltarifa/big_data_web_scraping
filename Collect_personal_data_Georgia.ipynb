{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fr-sbmiard"
      },
      "source": [
        "# Gerogia state and its depredators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Solve capchas\n",
        "*   Implement Selenium to dynamic handle of components\n",
        "\n"
      ],
      "metadata": {
        "id": "clfBB69h2As3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICD6o6RP39m"
      },
      "source": [
        "## 1 Gerogia state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92a_5rLvSUvI"
      },
      "source": [
        "### Installing libraries and creating the project's folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLyvXO8nP5LY"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy \n",
        "!scrapy startproject usa_personal_data\n",
        "!scrapy genspider -l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the spider"
      ],
      "metadata": {
        "id": "UMEBCaQv5lc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/usa_personal_data/usa_personal_data/spiders')\n",
        "!scrapy genspider -t crawl georgia_state https://state.sor.gbi.ga.gov/sort_public/SearchOffender.aspx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2bIB6J3KsN",
        "outputId": "55075f40-574f-4173-c72b-a2d45529061b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider 'georgia_state' already exists in module:\n",
            "  usa_personal_data.spiders.georgia_state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciVfL8tSQce"
      },
      "source": [
        "### Selenium / Scrapy class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing Selenium and dependences"
      ],
      "metadata": {
        "id": "KtvHK5Iz56xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_bi2xNUabG",
        "outputId": "c9fbe401-d91a-46e6-90cd-646e7fdb5542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-460\n",
            "0 upgraded, 0 newly installed, 1 to remove and 23 not upgraded.\n",
            "After this operation, 35.8 kB disk space will be freed.\n",
            "(Reading database ... 124139 files and directories currently installed.)\n",
            "Removing libnvidia-common-460 (460.106.00-0ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!apt autoremove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jARNHWqCpd2x"
      },
      "source": [
        "#### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43GA0LGCSWA0",
        "outputId": "ad50bbff-95ba-4671-85b8-501680455bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/usa_personal_data/usa_personal_data/spiders/class_selenium_xpath.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/class_selenium_xpath.py\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "class selenium_scrapy:\n",
        "  url_list=[]\n",
        "  def driversetup(self):\n",
        "      options = webdriver.ChromeOptions()\n",
        "      #run Selenium in headless mode\n",
        "      options.add_argument('--headless')\n",
        "      options.add_argument('--no-sandbox')\n",
        "      #overcome limited resource problems\n",
        "      options.add_argument('--disable-dev-shm-usage')\n",
        "      options.add_argument(\"lang=en\")\n",
        "      #open Browser in maximized mode\n",
        "      options.add_argument(\"start-maximized\")\n",
        "      #disable infobars\n",
        "      options.add_argument(\"disable-infobars\")\n",
        "      #disable extension\n",
        "      options.add_argument(\"--disable-extensions\")\n",
        "      options.add_argument(\"--incognito\")\n",
        "      options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "      prefs={'download.default_directory':'/content'}\n",
        "      options.add_experimental_option('prefs',prefs)\n",
        "      \n",
        "      driver = webdriver.Chrome(options=options)\n",
        "\n",
        "      driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\")\n",
        "\n",
        "      return driver\n",
        "    \n",
        "  def pagesource(self,url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "      #soup = BeautifulSoup(driver.page_source)\n",
        "      soup=driver.find_elements(By.XPATH, '//a[starts-with(@onclick, \"SetCapitu\")]' )\n",
        "      self.url_list = []\n",
        "      for a in soup:\n",
        "        self.url_list.append(self.url_title(a.get_attribute('onclick')))\n",
        "      driver.close()\n",
        "\n",
        "  def url_title(self,t):\n",
        "      replacements = [\n",
        "        (\"'\",\"\"),\n",
        "        (\")\",\"\"),\n",
        "        (\"(\",\"\"),\n",
        "        (\", \",\"/\"),\n",
        "        (\"SetCapitulo\",\"\"),\n",
        "      ]\n",
        "      for x,y in replacements:\n",
        "        t=t.replace(x,y)\n",
        "      return t+'/'\n",
        "    \n",
        "  def click_element_bcentral(self, url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "\n",
        "      #soup = BeautifulSoup(driver.page_source)\n",
        "      soup=driver.find_elements(By.XPATH, '//*[@id=\"fsTable\"]/div[1]/div[1]/button[5]' )[0]\n",
        "      soup.click()\n",
        "      button_before_click = soup.get_attribute('class')\n",
        "\n",
        "      soup=driver.find_elements(By.XPATH, '//*[@id=\"btnIQYContinue\"]' )[0]\n",
        "      #soup.click()\n",
        "      time.sleep(5)\n",
        "\n",
        "      button_after_click = soup.get_attribute('class')\n",
        "\n",
        "      return \"before_click >>> \" + str(button_before_click) + \"    after click >>\"+ str(button_after_click )\n",
        "      \n",
        "      driver.close()  \n",
        "\n",
        "  def click_exploring_ine(self, url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "\n",
        "      list_response = []\n",
        "      dict_file = {}\n",
        "\n",
        "      ## TITLE PANELS\n",
        "      arr_title = driver.find_elements(By.XPATH, '//*[@id=\"Content_C007_Col00\"]/div/div/div' )\n",
        "      for title in arr_title:\n",
        "        title.click()\n",
        "        time.sleep(3)\n",
        "\n",
        "        ## BUBTITLE PANELS\n",
        "        arr_subtitle = driver.find_elements(By.XPATH, '//*[@id=\"Content_C007_Col01\"]/div/div/div[4]/div/div/div' )\n",
        "        for subtitle in arr_subtitle:\n",
        "          subtitle.click()\n",
        "          time.sleep(3)\n",
        "\n",
        "          arr_link = driver.find_elements(By.XPATH, '//*[@id=\"Content\"]/div[3]/div[3]//a' )\n",
        "          for link in arr_link:\n",
        "\n",
        "            f_name = str(link.text)\n",
        "            *f_name,  format, size, dimension = re.split(r'[;,\\s]\\s*', f_name)\n",
        "            f_name = ' '.join(f_name)\n",
        "            \n",
        "            dict_file = {\n",
        "              'name' : title.text + \" / \" + subtitle.text, \n",
        "              'file_name':f_name,\n",
        "              'file_format':format,\n",
        "              'file_size':size,\n",
        "              'file_dimension':dimension,\n",
        "              \n",
        "              'link': link.get_attribute('href')\n",
        "            }\n",
        "            list_response.append(dict_file)\n",
        "\n",
        "      driver.close()  \n",
        "      return list_response\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEmiOKLpivo"
      },
      "source": [
        "#### Test of the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QPft6HjiwYp2"
      },
      "outputs": [],
      "source": [
        "#from chil_datacrawled_byinst.chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "#\n",
        "#extra_crawled_data = selenium_scrapy()\n",
        "#url='https://www.ine.gob.cl/estadisticas/economia/indices-de-precio-e-inflacion/indice-de-costos-del-transporte'\n",
        "#\n",
        "#list_response = extra_crawled_data.click_exploring_ine(url)\n",
        "#for dic in list_response:\n",
        "#  #print(dic)\n",
        "#  print(dic['name'])\n",
        "#  print(dic['file_name'])\n",
        "#  print(dic['file_format'])\n",
        "#  print(dic['file_size'])\n",
        "#  print(dic['file_dimension'])\n",
        "#  print(dic['link'])\n",
        "#  print(\"___________________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFKF6kwESgdf"
      },
      "source": [
        "### Spider code for GEORGIA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/BaseSORSpider.py\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import scrapy\n",
        "from scrapy import signals\n",
        "\n",
        "from ..models import IcrimewatchOffendersLinks\n",
        "from ..utils import (\n",
        "    OFFENDER_CARD,\n",
        "    addresses_keys,\n",
        "    basic_details_keys,\n",
        "    calculate_age,\n",
        "    clean_string,\n",
        "    generate_map_for_offender_card,\n",
        "    get_datetime_from_text,\n",
        "    physical_description_keys,\n",
        "    pop_redundant_data,\n",
        "    popout_offenses,\n",
        "    populate_offender_card_with_age_dob_info,\n",
        ")\n",
        "\n",
        "BASE_DIR = Path(__file__).resolve().parent.parent.parent\n",
        "\n",
        "class BaseSORSpider(scrapy.Spider):\n",
        "\n",
        "    name = 'sor'  # sex offender registry\n",
        "\n",
        "    def __init__(self, desired_domain=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.job_run_url = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler, *args, **kwargs):\n",
        "        spider = super(BaseSORSpider, cls).from_crawler(crawler, *args, **kwargs)\n",
        "        crawler.signals.connect(\n",
        "            spider.spider_opened,\n",
        "            signal=signals.spider_opened\n",
        "        )\n",
        "        return spider\n",
        "\n",
        "    def spider_opened(self):\n",
        "        if 'SHUB_JOBKEY' in os.environ:\n",
        "            crawler_url = os.environ.get('SHUB_JOBKEY')\n",
        "            self.job_run_url = f'http://app.zyte.com/p/{crawler_url}'\n",
        "        else:\n",
        "            self.job_run_url = 'Local run'\n",
        "\n",
        "    def parse(self, response, **kwargs):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseICrimeSpider(BaseSORSpider):\n",
        "    state_to_code = {\n",
        "        'Alabama': '54247',\n",
        "        'Connecticut': '54567',\n",
        "        'Indiana': '54812',\n",
        "        'Arizona': '55662',\n",
        "        'Louisiana': '54450',\n",
        "        'Maryland': '56622',\n",
        "        'NewJersey': '55260',\n",
        "        'NewMexico': '55290',\n",
        "        'Ohio': '55149',\n",
        "        'RhodeIsland': '56404',\n",
        "        'Utah': '54438',\n",
        "        'Vermont': '55275',\n",
        "        'Washington': '54528',\n",
        "        'Wyoming': '55699',\n",
        "    }\n",
        "\n",
        "    def obtain_session(self, response, **kwargs):\n",
        "        if 'sheriffalerts' in response.url:\n",
        "            fwd = response.xpath('//input[@name=\"fwd\"]/@value').get()\n",
        "            yield scrapy.FormRequest.from_response(\n",
        "                response,\n",
        "                formdata={\n",
        "                    \"fwd\": fwd,\n",
        "                    \"agree\": '1'\n",
        "                },\n",
        "                callback=self.parse_offender_personal_page,\n",
        "                dont_filter=True,\n",
        "            )\n",
        "        else:\n",
        "            yield next(self.parse_offender_personal_page(response))\n",
        "\n",
        "    def start_requests(self):\n",
        "        state = ''.join(self.name.split('_'))\n",
        "        query = (IcrimewatchOffendersLinks\n",
        "                 .select(IcrimewatchOffendersLinks.offender_link)\n",
        "                 .distinct()\n",
        "                 .where(IcrimewatchOffendersLinks.state == state))\n",
        "        offenders_links = [i.offender_link for i in list(query.execute())]\n",
        "        if offenders_links:\n",
        "            for offender in offenders_links:\n",
        "                yield scrapy.Request(\n",
        "                    offender,\n",
        "                    dont_filter=True,\n",
        "                    cb_kwargs={\n",
        "                        'state_code': self.state_to_code.get(state)\n",
        "                    },\n",
        "                    callback=self.obtain_session\n",
        "                )\n",
        "        else:\n",
        "            logging.error(f\"For selected state: {state}, there is no list of \"\n",
        "                          \"offenders' pages in the DataBase. Please scrape \"\n",
        "                          \"offenders' links first using \"\n",
        "                          \"js_script_for_icrimewatch!\")\n",
        "            self.close(self, 'No links in db')\n",
        "\n",
        "    def parse_offender_personal_page(self, response, **kwargs):\n",
        "        \"\"\"\n",
        "        Retrieving data from offender's page data structure should be\n",
        "        {\n",
        "        'url': '...',\n",
        "        'image_urls': [],\n",
        "        'Basic_Details': {data},\n",
        "        'Physical Description': {data},\n",
        "        'Address': 'String data',\n",
        "        'Offenses': {data}\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        data = []\n",
        "        for i in response.xpath('//div[@class=\"tabbertab\"]//tr//tr'):\n",
        "            s = i.xpath('./td//text()').getall()\n",
        "            data.append(s)\n",
        "        data = [clean_string(i) for i in data]\n",
        "        data = [i for i in data if len(i) != 0]\n",
        "        data = pop_redundant_data(data)\n",
        "        data = self.sorting_data(data)\n",
        "        data['url'] = response.url\n",
        "        data['image_urls'] = list(\n",
        "            set(response.xpath('//img[contains(@src,\"pictures\")]//@src').getall())\n",
        "        )\n",
        "        yield data\n",
        "\n",
        "    def sorting_data(self, data):\n",
        "        \"\"\"\n",
        "        Offenders page has really bad html code, lack of classes,\n",
        "        ids makes us create item more manually, checking if needed key in\n",
        "        list then making dict from list\n",
        "        \"\"\"\n",
        "        unstructured_data_list = [item for sublist in data for item in sublist]\n",
        "        if 'Vehicles' in unstructured_data_list:\n",
        "            unstructured_data_list = unstructured_data_list[:unstructured_data_list.index('Vehicles')]\n",
        "        for i in unstructured_data_list:\n",
        "            if '(DOB' in i:\n",
        "                index = unstructured_data_list.index(i)\n",
        "                age, dob = i.split('(DOB')\n",
        "                unstructured_data_list[index] = age.strip()\n",
        "                unstructured_data_list.insert(index+1, 'Date of Birth')\n",
        "                unstructured_data_list.insert(index + 2, dob.strip().replace(')', ''))\n",
        "            if '(YOB' in i:\n",
        "                index = unstructured_data_list.index(i)\n",
        "                age, year = i.split('(YOB')\n",
        "                if not age:\n",
        "                    age = str(calculate_age(get_datetime_from_text(year)))\n",
        "                unstructured_data_list[index] = age.strip()\n",
        "        unstructured_data_list, offenses_list = popout_offenses(unstructured_data_list)\n",
        "\n",
        "        keys, values = generate_map_for_offender_card(unstructured_data_list)\n",
        "        offender_card = copy.deepcopy(OFFENDER_CARD)\n",
        "\n",
        "        separated_offenses_list = []\n",
        "        offenses = []\n",
        "\n",
        "        self.group_rows(offenses, offenses_list, separated_offenses_list)\n",
        "\n",
        "        offender_card['offenses'] = offenses\n",
        "\n",
        "        for pair in zip(keys, values):\n",
        "            key = pair[0]\n",
        "            value = pair[1]\n",
        "            if key in basic_details_keys:\n",
        "                offender_card['basic_details'][key] = value\n",
        "            elif key in physical_description_keys:\n",
        "                offender_card['physical_description'][key] = value\n",
        "            elif key in addresses_keys:\n",
        "                offender_card['addresses'][key] = value\n",
        "            elif key == 'Warrants':\n",
        "                offender_card['warrants'].append(value)\n",
        "            elif key == 'Comments':\n",
        "                offender_card['comments'].append(value)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        offender_card = populate_offender_card_with_age_dob_info(offender_card)\n",
        "        offender_card['state'] = ' '.join(self.name.split('_'))\n",
        "\n",
        "        return offender_card\n",
        "\n",
        "    def group_rows(self, offenses, offenses_list, separated_offenses_list):\n",
        "        \"\"\"\n",
        "        not sure what it does, but it looks like it's a separate logic\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            if offenses_list:\n",
        "                try:\n",
        "                    counter = offenses_list.index(offenses_list[0], 1)\n",
        "                    while counter != 0:\n",
        "                        separated_offenses_list.append(offenses_list.pop(0))\n",
        "                        counter -= 1\n",
        "                except ValueError:\n",
        "                    counter = len(offenses_list)\n",
        "                    while counter != 0:\n",
        "                        separated_offenses_list.append(offenses_list.pop(0))\n",
        "                        counter -= 1\n",
        "                k, v = generate_map_for_offender_card(separated_offenses_list)\n",
        "                offenses_card = self.populate_offenses_card_template(k, v)\n",
        "                offenses.append(offenses_card)\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    @staticmethod\n",
        "    def populate_offenses_card_template(k, v):\n",
        "        offenses_card = copy.deepcopy(OFFENDER_CARD['offenses'][0])\n",
        "        for pair in zip(k, v):\n",
        "            try:\n",
        "                key = pair[0]\n",
        "                offenses_card[key] = pair[1]\n",
        "            except KeyError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                logging.error(e)\n",
        "        return "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJq5ZBdN-vp",
        "outputId": "6b521379-9741-4cd7-875d-4fa9050ba4b1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/usa_personal_data/usa_personal_data/spiders/BaseSORSpider.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 2captcha-python\n",
        "!pip install xvfbwrapper"
      ],
      "metadata": {
        "id": "Adp6MCivQ4OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a37e3130-3f78-4641-cb49-bb845d70e7c3",
        "id": "W2pAqbvvO6DV"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-fa1c6dc3f401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#from ..usa_personal_data.spiders import BaseSORSpider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from ..utils import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mOFFENDER_CARD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'settings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "##from chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/georgia_state.py\n",
        "import scrapy\n",
        "\n",
        "from scrapy.spiders import CrawlSpider,Rule\n",
        "from scrapy.linkextractors import LinkExtractor \n",
        "from scrapy import Selector\n",
        "import datetime\n",
        "\n",
        "from usa_personal_data.usa_personal_data.items import GeorgiaStateItem\n",
        "\n",
        "from usa_personal_data.usa_personal_data.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import scrapy\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from twocaptcha.solver import TwoCaptcha\n",
        "from xvfbwrapper import Xvfb\n",
        "\n",
        "from ..usa_personal_data.spiders import BaseSORSpider\n",
        "from ..settings import CAPTCHA_API_KEY\n",
        "from ..utils import (\n",
        "    OFFENDER_CARD,\n",
        "    addresses_keys,\n",
        "    basic_details_keys,\n",
        "    change_keys,\n",
        "    generate_keys_values_lists,\n",
        "    get_chromedriver,\n",
        "    physical_description_keys,\n",
        "    populate_offender_card_with_age_dob_info,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "class GerogiaSpider(CrawlSpider):\n",
        "    name = 'georgia_state'\n",
        "    allowed_domains = ['state.sor.gbi.ga.gov']\n",
        "    start_urls = ['http://state.sor.gbi.ga.gov/sort_public']\n",
        "    custom_settings = {\n",
        "        \"LOG_LEVEL\": \"DEBUG\",\n",
        "        \"HTTPCACHE_ENABLED\": False,\n",
        "        # \"DOWNLOADER_MIDDLEWARES\": {\n",
        "        #     \"sex_offender_registry.middlewares.ProxyMiddleware\": 500,\n",
        "        #     \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 543,\n",
        "        # }\n",
        "    }\n",
        "\n",
        "    counties_number = None\n",
        "    offenders_urls = []\n",
        "\n",
        "    cookies = None\n",
        "\n",
        "    headers = {\n",
        "        'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': '\"Linux\"',\n",
        "        'DNT': '1',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded',\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'host': 'apps.colorado.gov',\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_captcha_solution(driver):\n",
        "        sitekey = driver.find_element(\n",
        "            by=By.XPATH,\n",
        "            value='//div[@class=\"g-recaptcha\"]').get_attribute('data-sitekey')\n",
        "        solver = TwoCaptcha(CAPTCHA_API_KEY)\n",
        "        recaptcha_result = None\n",
        "\n",
        "        try:\n",
        "            recaptcha_result = solver.recaptcha(\n",
        "                sitekey=sitekey,\n",
        "                url=driver.current_url)\n",
        "        except Exception as e:\n",
        "            logging.error(e)\n",
        "        else:\n",
        "            logging.info('2Captcha result: ' + str(recaptcha_result))\n",
        "\n",
        "        if recaptcha_result:\n",
        "            captcha_code = recaptcha_result.get('code')\n",
        "            js_to_execute = f'document.getElementById(\"g-recaptcha-response\").innerHTML=\"{captcha_code}\"'\n",
        "            driver.execute_script(\n",
        "                'document.getElementById(\"g-recaptcha-response\").style.display = \"\"')\n",
        "            driver.execute_script(js_to_execute)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _get_offenders_urls(self):\n",
        "        url = 'https://apps.colorado.gov/apps/dps/sor/search/search-advanced.jsf'\n",
        "        vdisplay = Xvfb(width=1280, height=740, colordepth=16)\n",
        "        vdisplay.start()\n",
        "        county_number = 1\n",
        "        while True:\n",
        "            try:\n",
        "                driver = get_chromedriver(use_proxy=True, no_imgs=True)\n",
        "                driver.get(url)\n",
        "                driver.find_element(By.XPATH, value='//input[@id=\"acceptForm:submitLogIn\"]').click()\n",
        "                time.sleep(2)\n",
        "                select = Select(driver.find_element(By.ID, value='advancedSearchForm:county'))\n",
        "                if not self.counties_number:\n",
        "                    self.counties_number = len(select.options)\n",
        "                if self._get_captcha_solution(driver):\n",
        "                    select.select_by_index(county_number)\n",
        "                    logging.warning(select.first_selected_option.text)\n",
        "                    driver.find_element(By.XPATH, value='//input[@value=\"SEARCH\"]').click()\n",
        "                    offenders_in_county = driver.find_element(By.XPATH, value='//*[contains(text(), \"found\")]').text\n",
        "                    try:\n",
        "                        offenders_in_county = int(list(filter(lambda x: x.isnumeric(), offenders_in_county.split()))[0])\n",
        "                    except IndexError:\n",
        "                        logging.warning(offenders_in_county.replace('\\n', ' '))\n",
        "                        county_number += 1\n",
        "                        driver.close()\n",
        "                        continue\n",
        "                    logging.warning(offenders_in_county)\n",
        "                    i = 0\n",
        "                    while i < offenders_in_county:\n",
        "                        urls = driver.find_elements(By.XPATH, value='//a[contains(@href, \"search-detail\")]')\n",
        "                        self.offenders_urls += list(set([url.get_attribute('href') for url in urls]))\n",
        "                        if driver.find_elements(By.XPATH, value='//*[contains(@src, \"images/right\")]'):\n",
        "                            driver.find_element(By.XPATH, value='//*[contains(@src, \"images/right\")]').click()\n",
        "                        i += 25\n",
        "                if county_number < self.counties_number - 1:\n",
        "                    county_number += 1\n",
        "                    driver.close()\n",
        "                    continue\n",
        "                else:\n",
        "                    driver.close()\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                logging.error(e)\n",
        "                driver.close()\n",
        "                continue\n",
        "        vdisplay.stop()\n",
        "\n",
        "    def start_requests(self):\n",
        "        self._get_offenders_urls()\n",
        "        yield scrapy.Request(\n",
        "            url=self.offenders_urls[0],\n",
        "            cookies=self.cookies,\n",
        "            headers=self.headers\n",
        "        )\n",
        "\n",
        "    def parse(self, response, **kwargs):\n",
        "        yield scrapy.FormRequest.from_response(\n",
        "            response=response,\n",
        "            formxpath='//form[@id=\"acceptForm\"]',\n",
        "            callback=self.go_through_offenders_pages,\n",
        "            headers=self.headers\n",
        "        )\n",
        "\n",
        "    def go_through_offenders_pages(self, response):\n",
        "        for url in set(self.offenders_urls):\n",
        "            yield scrapy.Request(\n",
        "                url=url,\n",
        "                headers=self.headers,\n",
        "                callback=self.parse_offender_page,\n",
        "            )\n",
        "\n",
        "    def parse_offender_page(self, response):\n",
        "        oc = copy.deepcopy(OFFENDER_CARD)\n",
        "        name = list(s.xpath('normalize-space()').get() for s in response.xpath('//strong[contains(text(), \"Name:\")]/..'))\n",
        "        oc['basic_details']['Name'] = name[0].split(': ')[1]\n",
        "\n",
        "        main_address = list(s.xpath('normalize-space()').get() for s in\n",
        "                        response.xpath('//strong[contains(text(), \"Address:\")]/..'))\n",
        "        oc['addresses']['Address'] = main_address[0].split('Date')[0].split(': ')[1]\n",
        "\n",
        "        other_address = list(s.xpath('normalize-space()').get() for s in response.xpath('//div[@id=\"altLocations\"]//td'))\n",
        "        oc['addresses']['Other Residential Addresses'] = other_address\n",
        "\n",
        "        img_urls = response.xpath('//div[@class=\"detailsPhoto\"]//img/@src').getall()\n",
        "        img_urls = [response.urljoin(i) for i in img_urls]\n",
        "        oc['image_urls'] = img_urls\n",
        "\n",
        "        pd = list(s.xpath('normalize-space()').get() for s in response.xpath('//h4[text()=\"DESCRIPTION:\"]//following-sibling::p'))\n",
        "        pd_keys, pd_values = generate_keys_values_lists(pd)\n",
        "        pd_keys = change_keys(pd_keys, self.name)\n",
        "\n",
        "        for pair in zip(pd_keys, pd_values):\n",
        "            key = pair[0]\n",
        "            value = pair[1]\n",
        "            if key in basic_details_keys:\n",
        "                oc[\"basic_details\"][key] = value\n",
        "            elif key in physical_description_keys:\n",
        "                oc[\"physical_description\"][key] = value\n",
        "            elif key in addresses_keys:\n",
        "                oc[\"addresses\"][key] = value\n",
        "            elif key == \"Warrants\":\n",
        "                oc[\"warrants\"].append(value)\n",
        "            elif key == \"Comments\":\n",
        "                oc[\"comments\"].append(value)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        smt = list(s.xpath('normalize-space()').get() for s in response.xpath('//table[@id=\"detailSmtTable\"]//td'))\n",
        "        oc['physical_description']['Scars/Tattoos'] = smt\n",
        "\n",
        "        offenses_list = list(s.xpath('normalize-space()').get() for s in response.xpath('//table[@id=\"convictionsDetailTable\"]//td'))\n",
        "        offenses = []\n",
        "        for i in range(0, len(offenses_list), 3):\n",
        "            offense = copy.deepcopy(OFFENDER_CARD['offenses'][0])\n",
        "            offense['Details'] = offenses_list[i]\n",
        "            offense['Description'] = offenses_list[i+1]\n",
        "            offense['Date Convicted'] = offenses_list[i+2]\n",
        "            offenses.append(offense)\n",
        "        oc['offenses'] = offenses\n",
        "\n",
        "        reg_num = response.url.split('=')[-1]\n",
        "        oc['basic_details']['Registration #'] = reg_num\n",
        "\n",
        "        aliases = list(s.xpath('normalize-space()').get() for s in response.xpath('//table[@id=\"aliases\"]//td'))\n",
        "        oc['basic_details']['Aliases'] = aliases\n",
        "\n",
        "        oc = populate_offender_card_with_age_dob_info(oc)\n",
        "        if oc.get('age'):\n",
        "            oc['physical_description']['Age'] = oc['age']\n",
        "\n",
        "        oc['state'] = ' '.join(self.name.split('_'))\n",
        "        oc['url'] = response.url\n",
        "\n",
        "        yield oc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0SO2OmAXL2a"
      },
      "source": [
        "### Editing Item Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F542tXycXU2b",
        "outputId": "9ef9cad8-8dcf-4dcf-ae69-dbba732384ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/usa_personal_data/usa_personal_data/items.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/usa_personal_data/usa_personal_data/items.py\n",
        "import scrapy\n",
        "class GeorgiaStateItem(scrapy.Item):\n",
        "    \n",
        "    ############ basic_details {\n",
        "    Name = scrapy.Field()\n",
        "    Registration  = scrapy.Field()\n",
        "    Last_Verification_Date = scrapy.Field()\n",
        "    Aliases = scrapy.Field()\n",
        "    Level = scrapy.Field()\n",
        "    Status = scrapy.Field()\n",
        "    Registrant_Type = scrapy.Field()\n",
        "    Registration_Start_Date = scrapy.Field()\n",
        "    Registration_End_Date = scrapy.Field()\n",
        "    Lifetime_Registration = scrapy.Field()\n",
        "    ############ } basic_details\n",
        "\n",
        "    #file_urls = scrapy.Field()\n",
        "    #files = scrapy.Field\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OFFENDER_CARD = {\n",
        "    'basic_details': {\n",
        "        'Name': None,\n",
        "        'Registration #': None,\n",
        "        'Last Verification Date': None,\n",
        "        'Aliases': None,\n",
        "        'Level': None,\n",
        "        'Status': None,\n",
        "        'Registrant Type': None,\n",
        "        'Registration Start Date': None,\n",
        "        'Registration End Date': None,\n",
        "        'Lifetime Registration': None,\n",
        "    },\n",
        "    'physical_description': {\n",
        "        'Age': None,\n",
        "        'Date of Birth': None,\n",
        "        'Height': None,\n",
        "        'Sex': None,\n",
        "        'Weight': None,\n",
        "        'Race': None,\n",
        "        'Eyes': None,\n",
        "        'Hair': None,\n",
        "        'Scars/Tattoos': None,\n",
        "    },\n",
        "    'addresses': {\n",
        "        'Address': None,\n",
        "        'Work Addresses': None,\n",
        "        'School Addresses': None,\n",
        "        'Volunteer Addresses': None,\n",
        "        'Other Residential Addresses': None,\n",
        "    },\n",
        "    'offenses': [\n",
        "        {\n",
        "            'Description': None,\n",
        "            'Date Convicted': None,\n",
        "            'Conviction State': None,\n",
        "            'Release Date': None,\n",
        "            'Details': None,\n",
        "            'County of Conviction': None,\n",
        "            'Case Number': None,\n",
        "            'Sentence': None,\n",
        "            'State': None,\n",
        "            'Probation Conditions': None,\n",
        "        }\n",
        "    ],\n",
        "    'warrants': [],\n",
        "    'comments': [],\n",
        "    'image_urls': [],\n",
        "    'age': None,\n",
        "    'date_of_birth': None,\n",
        "    'date_of_birth_raw': None,\n",
        "    'state': None,\n",
        "}"
      ],
      "metadata": {
        "id": "AMUbe2Sb7Dtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk_B22EPh9Lf"
      },
      "source": [
        "### Setting.py conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLM_wpKuiBnY",
        "outputId": "a1527197-8332-4f7e-91b2-ddadd84b4e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/usa_personal_data/usa_personal_data/settings.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a /content/usa_personal_data/usa_personal_data/settings.py\n",
        "\n",
        "#ITEM_PIPELINES = {\n",
        "#  'scrapy.pipelines.files.FilesPipeline': 1,\n",
        "#}\n",
        "\n",
        "FILES_URLS_FIELD = 'file_urls'\n",
        "FILES_RESULT_FIELD = 'files'\n",
        "\n",
        "# 120 days of delay for files expiration\n",
        "FILES_EXPIRES = 120\n",
        "\n",
        "\n",
        "\n",
        "PROXY_LIST = [\n",
        "    \"https://giovwwwl-rotate:dc7luz5au5k5@p.webshare.io:80\",  # 1000 US proxies from webshare\n",
        "]\n",
        "CAPTCHA_API_KEY = \"8319c528f24d11664e92f4aba241d413\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IvETMVqTfYO"
      },
      "source": [
        "### Exec 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GQTsWD2FTebn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7855af82-4551-414a-a31d-119e4598dabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-11 04:49:23 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: usa_personal_data)\n",
            "2023-01-11 04:49:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.16 (default, Dec  7 2022, 01:12:13) - [GCC 7.5.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 39.0.0, Platform Linux-5.10.147+-x86_64-with-glibc2.27\n",
            "2023-01-11 04:49:24 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'usa_personal_data',\n",
            " 'NEWSPIDER_MODULE': 'usa_personal_data.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['usa_personal_data.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-01-11 04:49:24 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-01-11 04:49:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-01-11 04:49:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-01-11 04:49:24 [scrapy.extensions.telnet] INFO: Telnet Password: bc772edbc10e8f5b\n",
            "2023-01-11 04:49:24 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-01-11 04:49:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-01-11 04:49:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-01-11 04:49:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-01-11 04:49:24 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-01-11 04:49:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-01-11 04:49:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-01-11 04:49:24 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://state.sor.gbi.ga.gov/robots.txt> from <GET http://state.sor.gbi.ga.gov/robots.txt>\n",
            "2023-01-11 04:49:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://state.sor.gbi.ga.gov/robots.txt> (referer: None)\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 8 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 9 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 10 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 11 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.\n",
            "2023-01-11 04:49:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://state.sor.gbi.ga.gov/sort_public> from <GET http://state.sor.gbi.ga.gov/sort_public>\n",
            "2023-01-11 04:49:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://state.sor.gbi.ga.gov/sort_public/> from <GET https://state.sor.gbi.ga.gov/sort_public>\n",
            "2023-01-11 04:49:25 [filelock] DEBUG: Attempting to acquire lock 139831532283456 on /root/.cache/python-tldextract/3.8.16.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2023-01-11 04:49:25 [filelock] DEBUG: Lock 139831532283456 acquired on /root/.cache/python-tldextract/3.8.16.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2023-01-11 04:49:25 [filelock] DEBUG: Attempting to release lock 139831532283456 on /root/.cache/python-tldextract/3.8.16.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2023-01-11 04:49:25 [filelock] DEBUG: Lock 139831532283456 released on /root/.cache/python-tldextract/3.8.16.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2023-01-11 04:49:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://state.sor.gbi.ga.gov/sort_public/> (referer: None)\n",
            "2023-01-11 04:49:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-01-11 04:49:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 1154,\n",
            " 'downloader/request_count': 5,\n",
            " 'downloader/request_method_count/GET': 5,\n",
            " 'downloader/response_bytes': 11526,\n",
            " 'downloader/response_count': 5,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/301': 1,\n",
            " 'downloader/response_status_count/302': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 1.319255,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 1, 11, 4, 49, 25, 474264),\n",
            " 'log_count/DEBUG': 19,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 77717504,\n",
            " 'memusage/startup': 77717504,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 3,\n",
            " 'scheduler/dequeued/memory': 3,\n",
            " 'scheduler/enqueued': 3,\n",
            " 'scheduler/enqueued/memory': 3,\n",
            " 'start_time': datetime.datetime(2023, 1, 11, 4, 49, 24, 155009)}\n",
            "2023-01-11 04:49:25 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy crawl georgia_state # -O file_storing_INE.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxYGkAmtR9AQ",
        "outputId": "ef989904-0ac4-408f-9c64-9ec74aa569a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/usa_personal_data/usa_personal_data/spiders/georgia_state.py\n"
          ]
        }
      ],
      "source": [
        "##from chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/georgia_state.py\n",
        "import scrapy\n",
        "\n",
        "from scrapy.spiders import CrawlSpider,Rule\n",
        "from scrapy.linkextractors import LinkExtractor \n",
        "from scrapy import Selector\n",
        "import datetime\n",
        "\n",
        "from usa_personal_data.items import GeorgiaStateItem\n",
        "\n",
        "from usa_personal_data.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "class GerogiaSpider(CrawlSpider):\n",
        "    name = 'georgia_state'\n",
        "    allowed_domains = ['state.sor.gbi.ga.gov']\n",
        "    start_urls = ['http://state.sor.gbi.ga.gov/sort_public']\n",
        "\n",
        "    link_extractor = LinkExtractor(allow=r'/Captcha.aspx')\n",
        "\n",
        "    rules = (\n",
        "        Rule(link_extractor, callback='parse_item', follow=True),\n",
        "    )\n",
        "\n",
        "    def parse_item(self, response):\n",
        "      #file = GeorgiaStateItem()\n",
        "      \n",
        "      search = response.xpath('//*[@id=\"mymenu\"]/li[1]/a').get()\n",
        "      data = {\n",
        "        'search' : response.url\n",
        "\n",
        "      }\n",
        "\n",
        "      \n",
        "      yield data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tQmxJTMZLtzW",
        "w27N3RGwI1F0",
        "pA5FqcRkISfk",
        "RDuADs6crNO7",
        "mq9RIExW_JYP",
        "a8mBDBR1azgc",
        "0GY-Ynl3tY_2",
        "ULNiZNh8P08d",
        "92a_5rLvSUvI",
        "FciVfL8tSQce",
        "cFKF6kwESgdf",
        "J0SO2OmAXL2a"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOoLcrHCg+M0ZtXcU/dmnHd"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}