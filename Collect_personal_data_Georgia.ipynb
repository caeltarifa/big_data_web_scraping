{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fr-sbmiard"
      },
      "source": [
        "# Gerogia state and its depredators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Solve capchas\n",
        "*   Implement Selenium to dynamic handle of components\n",
        "\n"
      ],
      "metadata": {
        "id": "clfBB69h2As3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ICD6o6RP39m"
      },
      "source": [
        "##B Gerogia state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92a_5rLvSUvI"
      },
      "source": [
        "### Initializing project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLyvXO8nP5LY"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy \n",
        "!scrapy startproject usa_personal_data\n",
        "!scrapy genspider -l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/usa_personal_data/usa_personal_data/spiders')\n",
        "!scrapy genspider -t crawl georgia_state https://state.sor.gbi.ga.gov/sort_public/SearchOffender.aspx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2bIB6J3KsN",
        "outputId": "622d95c2-713f-466b-d8b2-484220db7a89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'georgia_state' using template 'crawl' in module:\n",
            "  usa_personal_data.spiders.georgia_state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciVfL8tSQce"
      },
      "source": [
        "### Selenium / Scrapy class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o_bi2xNUabG"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!apt autoremove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jARNHWqCpd2x"
      },
      "source": [
        "#### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43GA0LGCSWA0",
        "outputId": "af411e47-e3ac-400a-89be-9f7102f10ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/chil_datacrawled_byinst/chil_datacrawled_byinst/spiders/class_selenium_xpath.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/class_selenium_xpath.py\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "class selenium_scrapy:\n",
        "  url_list=[]\n",
        "  def driversetup(self):\n",
        "      options = webdriver.ChromeOptions()\n",
        "      #run Selenium in headless mode\n",
        "      options.add_argument('--headless')\n",
        "      options.add_argument('--no-sandbox')\n",
        "      #overcome limited resource problems\n",
        "      options.add_argument('--disable-dev-shm-usage')\n",
        "      options.add_argument(\"lang=en\")\n",
        "      #open Browser in maximized mode\n",
        "      options.add_argument(\"start-maximized\")\n",
        "      #disable infobars\n",
        "      options.add_argument(\"disable-infobars\")\n",
        "      #disable extension\n",
        "      options.add_argument(\"--disable-extensions\")\n",
        "      options.add_argument(\"--incognito\")\n",
        "      options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "      prefs={'download.default_directory':'/content'}\n",
        "      options.add_experimental_option('prefs',prefs)\n",
        "      \n",
        "      driver = webdriver.Chrome(options=options)\n",
        "\n",
        "      driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\")\n",
        "\n",
        "      return driver\n",
        "    \n",
        "  def pagesource(self,url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "      #soup = BeautifulSoup(driver.page_source)\n",
        "      soup=driver.find_elements(By.XPATH, '//a[starts-with(@onclick, \"SetCapitu\")]' )\n",
        "      self.url_list = []\n",
        "      for a in soup:\n",
        "        self.url_list.append(self.url_title(a.get_attribute('onclick')))\n",
        "      driver.close()\n",
        "\n",
        "  def url_title(self,t):\n",
        "      replacements = [\n",
        "        (\"'\",\"\"),\n",
        "        (\")\",\"\"),\n",
        "        (\"(\",\"\"),\n",
        "        (\", \",\"/\"),\n",
        "        (\"SetCapitulo\",\"\"),\n",
        "      ]\n",
        "      for x,y in replacements:\n",
        "        t=t.replace(x,y)\n",
        "      return t+'/'\n",
        "    \n",
        "  def click_element_bcentral(self, url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "\n",
        "      #soup = BeautifulSoup(driver.page_source)\n",
        "      soup=driver.find_elements(By.XPATH, '//*[@id=\"fsTable\"]/div[1]/div[1]/button[5]' )[0]\n",
        "      soup.click()\n",
        "      button_before_click = soup.get_attribute('class')\n",
        "\n",
        "      soup=driver.find_elements(By.XPATH, '//*[@id=\"btnIQYContinue\"]' )[0]\n",
        "      #soup.click()\n",
        "      time.sleep(5)\n",
        "\n",
        "      button_after_click = soup.get_attribute('class')\n",
        "\n",
        "      return \"before_click >>> \" + str(button_before_click) + \"    after click >>\"+ str(button_after_click )\n",
        "      \n",
        "      driver.close()  \n",
        "\n",
        "  def click_exploring_ine(self, url):\n",
        "      driver = self.driversetup()\n",
        "      driver.get(url)\n",
        "\n",
        "      list_response = []\n",
        "      dict_file = {}\n",
        "\n",
        "      ## TITLE PANELS\n",
        "      arr_title = driver.find_elements(By.XPATH, '//*[@id=\"Content_C007_Col00\"]/div/div/div' )\n",
        "      for title in arr_title:\n",
        "        title.click()\n",
        "        time.sleep(3)\n",
        "\n",
        "        ## BUBTITLE PANELS\n",
        "        arr_subtitle = driver.find_elements(By.XPATH, '//*[@id=\"Content_C007_Col01\"]/div/div/div[4]/div/div/div' )\n",
        "        for subtitle in arr_subtitle:\n",
        "          subtitle.click()\n",
        "          time.sleep(3)\n",
        "\n",
        "          arr_link = driver.find_elements(By.XPATH, '//*[@id=\"Content\"]/div[3]/div[3]//a' )\n",
        "          for link in arr_link:\n",
        "\n",
        "            f_name = str(link.text)\n",
        "            *f_name,  format, size, dimension = re.split(r'[;,\\s]\\s*', f_name)\n",
        "            f_name = ' '.join(f_name)\n",
        "            \n",
        "            dict_file = {\n",
        "              'name' : title.text + \" / \" + subtitle.text, \n",
        "              'file_name':f_name,\n",
        "              'file_format':format,\n",
        "              'file_size':size,\n",
        "              'file_dimension':dimension,\n",
        "              \n",
        "              'link': link.get_attribute('href')\n",
        "            }\n",
        "            list_response.append(dict_file)\n",
        "\n",
        "      driver.close()  \n",
        "      return list_response\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEmiOKLpivo"
      },
      "source": [
        "#### Test of the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPft6HjiwYp2"
      },
      "outputs": [],
      "source": [
        "#from chil_datacrawled_byinst.chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "#\n",
        "#extra_crawled_data = selenium_scrapy()\n",
        "#url='https://www.ine.gob.cl/estadisticas/economia/indices-de-precio-e-inflacion/indice-de-costos-del-transporte'\n",
        "#\n",
        "#list_response = extra_crawled_data.click_exploring_ine(url)\n",
        "#for dic in list_response:\n",
        "#  #print(dic)\n",
        "#  print(dic['name'])\n",
        "#  print(dic['file_name'])\n",
        "#  print(dic['file_format'])\n",
        "#  print(dic['file_size'])\n",
        "#  print(dic['file_dimension'])\n",
        "#  print(dic['link'])\n",
        "#  print(\"___________________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFKF6kwESgdf"
      },
      "source": [
        "### Spider code for GEORGIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxYGkAmtR9AQ",
        "outputId": "700bcfc9-e2c8-4fe6-ac60-a6436a9bdd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/chil_datacrawled_byinst/chil_datacrawled_byinst/spiders/bancocentral_cl.py\n"
          ]
        }
      ],
      "source": [
        "##from chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "%%writefile /content/usa_personal_data/usa_personal_data/spiders/georgia_state.py\n",
        "import scrapy\n",
        "\n",
        "from scrapy.spiders import CrawlSpider,Rule\n",
        "from scrapy.linkextractors import LinkExtractor \n",
        "from scrapy import Selector\n",
        "import datetime\n",
        "\n",
        "from chil_datacrawled_byinst.items import ChilDatacrawledByinstItem\n",
        "\n",
        "from chil_datacrawled_byinst.spiders.class_selenium_xpath import selenium_scrapy\n",
        "\n",
        "class BancocentralClSpider(CrawlSpider):\n",
        "    name = 'georgia_state'\n",
        "    allowed_domains = ['state.sor.gbi.ga.gov']\n",
        "    start_urls = ['http://state.sor.gbi.ga.gov/']\n",
        "\n",
        "    link_extractor = LinkExtractor(allow=r'')\n",
        "\n",
        "    rules = (\n",
        "        Rule(link_extractor, callback='parse_item', follow=True),\n",
        "    )\n",
        "\n",
        "    def parse_item(self, response):\n",
        "      A = \"//a[contains(@href,'xls') or contains(@href,'pdf') or contains(@href, 'csv') or contains(@href, 'dta') or contains(@href, 'sav')]\" \n",
        "      B = \"//span[contains(@class, 'titulo')]/text() \" \n",
        "      B = \"//a/*[last()]/text() \"\n",
        "      C = '//*[@id=\"Content\"]/div[3]/div'\n",
        "\n",
        "      file_url = response.xpath(A)\n",
        "\n",
        "      if file_url:\n",
        "        a_nodes = file_url.xpath(A).getall()\n",
        "        a_nodes = self.remove_duplicates(a_nodes)\n",
        "        names__list = []\n",
        "\n",
        "        for a in a_nodes:\n",
        "          new_node = Selector(text=a)\n",
        "          childs = new_node.xpath(\"//a/*\")\n",
        "          sw=\"false\"\n",
        "          \n",
        "          if len(childs.getall())>=1:\n",
        "            name_file_childs = new_node.xpath(B).getall()[0]\n",
        "          else:\n",
        "            name_file_childs = new_node.xpath(\"//a/text()\").getall()[0]\n",
        "\n",
        "          names__list.append(\n",
        "            ( \n",
        "              response.urljoin(new_node.xpath('//a/@href').getall()[0]),\n",
        "              name_file_childs\n",
        "            )\n",
        "          )\n",
        "        \n",
        "          url_file_store = response.urljoin(response.urljoin(new_node.xpath('//a/@href').getall()[0]))\n",
        "          url_file_store = url_file_store if not '?' in url_file_store else url_file_store.split('?')[0]\n",
        "          \n",
        "          file = ChilDatacrawledByinstItem()\n",
        "          file['date'] = str(datetime.datetime.now().date()) \n",
        "          file['time'] = str(datetime.datetime.now().time().strftime('%H:%M:%S')) \n",
        "          file['root_url'] = response.url\n",
        "          file['file_title_web'] = name_file_childs\n",
        "          file['file_stored_name'] = url_file_store.split('/')[-1]\n",
        "          file['file_down_url'] = url_file_store\n",
        "          file['file_format'] = url_file_store.split('/')[-1].split('.')[1]\n",
        "          file['file_urls'] = [url_file_store]\n",
        "\n",
        "          yield file\n",
        "        \n",
        "        #Collecting files from dynamic events into panels\n",
        "        file_url = response.xpath(C)\n",
        "        if file_url:\n",
        "          extra_crawled_data = selenium_scrapy()\n",
        "          list_response = extra_crawled_data.click_exploring_ine(response.url)\n",
        "\n",
        "          for dict in list_response:\n",
        "            file = ChilDatacrawledByinstItem()\n",
        "            file['root_url'] = response.url\n",
        "            file['date'] = str(datetime.datetime.now().date()) \n",
        "            file['time'] = str(datetime.datetime.now().time().strftime('%H:%M:%S')) \n",
        "            file['file_title_web'] = dict['name']\n",
        "            file['file_stored_name'] = dict['file_name']\n",
        "            file['file_down_url'] = dict['link']\n",
        "            file['file_format'] = dict['file_format']\n",
        "            file['file_size'] = dict['file_size']\n",
        "            file['file_dimension'] = dict['file_dimension']\n",
        "            file['file_urls'] = [dict['link']]\n",
        "\n",
        "            yield file\n",
        "\n",
        "    def remove_duplicates(self, list):\n",
        "      set_ = set()\n",
        "      for i in list:\n",
        "        if not i in set_:\n",
        "          set_.add(str(i).strip())\n",
        "      return sorted(set_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0SO2OmAXL2a"
      },
      "source": [
        "### Editing Item Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F542tXycXU2b",
        "outputId": "f1d18ccd-a8d9-489b-a999-cf135a8a291b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/chil_datacrawled_byinst/chil_datacrawled_byinst/items.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/chil_datacrawled_byinst/chil_datacrawled_byinst/items.py\n",
        "import scrapy\n",
        "class ChilDatacrawledByinstItem(scrapy.Item):\n",
        "    root_url = scrapy.Field()\n",
        "    date = scrapy.Field()\n",
        "    time = scrapy.Field()\n",
        "    file_title_web = scrapy.Field()\n",
        "    file_stored_name = scrapy.Field()\n",
        "    file_down_url = scrapy.Field()\n",
        "\n",
        "    file_format = scrapy.Field()\n",
        "    file_size = scrapy.Field()\n",
        "    file_dimension = scrapy.Field()\n",
        "    \n",
        "    file_urls = scrapy.Field()\n",
        "    files = scrapy.Field\n",
        "    #pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk_B22EPh9Lf"
      },
      "source": [
        "### Setting.py conf for Storing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install botocore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT5UAxIxRsse",
        "outputId": "155a0789-1a5c-4761-d85b-9b77cb5771c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting botocore\n",
            "  Downloading botocore-1.29.26-py3-none-any.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from botocore) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore) (1.26.13)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.15.0)\n",
            "Installing collected packages: botocore\n",
            "Successfully installed botocore-1.29.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLM_wpKuiBnY",
        "outputId": "53901a63-fa6b-4528-9eac-828266575f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/chil_datacrawled_byinst/chil_datacrawled_byinst/settings.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a /content/chil_datacrawled_byinst/chil_datacrawled_byinst/settings.py\n",
        "\n",
        "#ITEM_PIPELINES = {\n",
        "#  'scrapy.pipelines.files.FilesPipeline': 1,\n",
        "#}\n",
        "\n",
        "FILES_URLS_FIELD = 'file_urls'\n",
        "FILES_RESULT_FIELD = 'files'\n",
        "\n",
        "# 120 days of delay for files expiration\n",
        "FILES_EXPIRES = 120\n",
        "\n",
        "#process broken responses\n",
        "DOWNLOAD_FAIL_ON_DATALOSS = True\n",
        "\n",
        "# Allowing redirections\n",
        "MEDIA_ALLOW_REDIRECTS = True\n",
        "\n",
        "\n",
        "ITEM_PIPELINES = {\n",
        "  #'chil_datacrawled_byinst.pipelines.ChilDatacrawledByinstPipeline': 300,\n",
        "  #'scrapy.pipelines.files.FilesPipeline': 1,\n",
        "}\n",
        "\n",
        "# AWS settings\n",
        "\n",
        "#FILES_STORE = r\"/content/ine_cl\"\n",
        "#FILES_STORE = 's3://biosoft-collection/' #'s3:::biosoft-collection'   \n",
        "#AWS_ACCESS_KEY_ID = 'AKIAWW7ABYZWPO7CLBE2'\n",
        "#AWS_SECRET_ACCESS_KEY= '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ro3XlGjToU"
      },
      "source": [
        "### Pipelines conf for downloading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60iqOU7njiqn",
        "outputId": "d1215903-cafb-4be5-e0f4-5d222ace562f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/chil_datacrawled_byinst/chil_datacrawled_byinst/pipelines.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a /content/chil_datacrawled_byinst/chil_datacrawled_byinst/pipelines.py\n",
        "import hashlib\n",
        "from scrapy.pipelines.files import FilesPipeline\n",
        "  \n",
        "class ChilDatacrawledByinstPipeline(FilesPipeline):\n",
        "    def file_path(self, request, response=None, info=None):\n",
        "        #file_name: str = request.url.split(\"/\")[-1]\n",
        "        #return file_name\n",
        "        \n",
        "        doc_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)\n",
        "        doc_perspective = request.url.split('/')[-1]\n",
        "        domain= request.url.split('/')[2]\n",
        "        file_name: str = f'{doc_url_hash}_{domain}_{doc_perspective}'\n",
        "\n",
        "        return file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IvETMVqTfYO"
      },
      "source": [
        "### Exec 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQTsWD2FTebn"
      },
      "outputs": [],
      "source": [
        "!scrapy crawl bancocentral_cl -O file_storing_INE.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tQmxJTMZLtzW",
        "w27N3RGwI1F0",
        "pA5FqcRkISfk",
        "RDuADs6crNO7",
        "mq9RIExW_JYP",
        "a8mBDBR1azgc",
        "0GY-Ynl3tY_2",
        "ULNiZNh8P08d",
        "92a_5rLvSUvI",
        "FciVfL8tSQce",
        "cFKF6kwESgdf",
        "J0SO2OmAXL2a"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM7G7kyljCm7QXJj7+GFi1v"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}